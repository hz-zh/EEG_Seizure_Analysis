{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Connect google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1wzM46IZiF9",
        "outputId": "6a038907-0500-4fe3-f0a5-18d5c0e6dd34"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyriemann"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qu2hPTX7Z4Q0",
        "outputId": "265ac297-0571-4e70-dcd0-9287c56e76d7"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyriemann in /usr/local/lib/python3.10/dist-packages (0.5)\n",
            "Requirement already satisfied: numpy!=1.24.0 in /usr/local/lib/python3.10/dist-packages (from pyriemann) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyriemann) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pyriemann) (1.2.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from pyriemann) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyriemann) (3.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PKggDf7ZJZK",
        "outputId": "2623aa05-e42b-4ded-f1f1-8309b5ce4fc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data shape is (37666, 23, 256)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "# Get the path to your Google Drive\n",
        "drive_path = \"/content/drive/My Drive\"\n",
        "\n",
        "# Combine the path to your Google Drive with the path to your data\n",
        "local_path = os.path.join(drive_path, \"EEG_Seizure_Analysis/Data/\")\n",
        "\n",
        "# Set the local path\n",
        "os.chdir('.')\n",
        "\n",
        "# Get the data\n",
        "npz_file = np.load(f'{local_path}eeg-seizure_train.npz', allow_pickle=True)\n",
        "signals_train = npz_file['train_signals']\n",
        "labels_train = npz_file['train_labels']\n",
        "\n",
        "npz_file = np.load(f'{local_path}eeg-seizure_val.npz', allow_pickle=True)\n",
        "signals_val = npz_file['val_signals']\n",
        "labels_val = npz_file['val_labels']\n",
        "\n",
        "npz_file = np.load(f'{local_path}eeg-seizure_test.npz', allow_pickle=True)\n",
        "signals_test = npz_file['test_signals']\n",
        "\n",
        "\n",
        "print('Train data shape is {}'.format(signals_train.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6wfKAX6ZJZL",
        "outputId": "b63af0c2-c9f2-45bd-a599-d91cbc2ad090"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1]\n",
            "0 1\n"
          ]
        }
      ],
      "source": [
        "print(labels_train[0:25])\n",
        "print(labels_train.min(), labels_train.max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtRcCryOZJZM",
        "outputId": "711dc454-4224-4859-d186-43421f6ff5da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of the first signal in samples: 37666\n"
          ]
        }
      ],
      "source": [
        "print(f'length of the first signal in samples: {signals_train.shape[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmN40u6AZJZM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "# Load the data from .npz files\n",
        "\n",
        "nr_samples = signals_train.shape[0]\n",
        "cases = 23\n",
        "sampling_rate = 256  # in Hz\n",
        "time = np.linspace(0, 1, sampling_rate, endpoint=False)  # 1 second of data\n",
        "\n",
        "# Function to plot EEG signals\n",
        "def plot_eeg_signals(signals, examples=[0, 1], channels=[0]):\n",
        "    for i in examples:\n",
        "        fig = go.Figure()\n",
        "        for j, channel in enumerate(channels):\n",
        "            fig.add_trace(go.Scatter(x=time, y=signals[i, channel, :], mode='lines', name=f\"Channel {channel}\"))\n",
        "        # Include the label (true, false) in the title\n",
        "        fig.update_layout(height=600, width=1000, title_text=f\"EEG Signal Waveforms for Example {i} – Seizure: {'TRUE' if labels_train[i] == 1 else 'FALSE'}\", title_x=0.5, xaxis_title='Time (s)', yaxis_title='Amplitude (µV)')\n",
        "        fig.show()\n",
        "\n",
        "# Function to calculate basic statistics\n",
        "def calculate_statistics(signals):\n",
        "    means = np.mean(signals, axis=(1, 2))\n",
        "    variances = np.var(signals, axis=(1, 2))\n",
        "    return means, variances\n",
        "\n",
        "# Function to check for missing or corrupted data\n",
        "def check_data(signals):\n",
        "    # find channels with zero variance (dead channels which we can interpolate or remove)\n",
        "    zero_variance_channels = []\n",
        "    for i in range(signals.shape[0]):\n",
        "        for j in range(signals.shape[1]):\n",
        "            if np.var(signals[i, j, :]) == 0:\n",
        "                #print(f\"Channel {j} in example {i} has zero variance.\")\n",
        "                zero_variance_channels.append((i, j))\n",
        "\n",
        "    if not np.all(np.isfinite(signals)):\n",
        "        print(\"Data contains non-finite values (inf or nan).\")\n",
        "    if not np.all(np.isreal(signals)):\n",
        "        print(\"Data contains complex numbers.\")\n",
        "    if np.any(np.isnan(signals)):\n",
        "        print(\"Data contains NaN values.\")\n",
        "    if len(zero_variance_channels) > 0:\n",
        "        print(f\"Data contains channels with zero variance: {zero_variance_channels}\")\n",
        "        print(f'Number of channels with zero variance: {len(zero_variance_channels)}')\n",
        "        print(f'Average number of channels with zero variance per example: {np.round((len(zero_variance_channels) / signals.shape[0]), 2)}')\n",
        "    else:\n",
        "        print(\"Data is clean.\")\n",
        "\n",
        "# Plot the signals\n",
        "plot_eeg_signals(signals_train, examples=range(0, 20), channels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22])\n",
        "\n",
        "# Check data for issues\n",
        "#check_data(signals_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqqCsMsoZJZM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.fftpack import fft\n",
        "\n",
        "\n",
        "nr_samples = signals_train.shape[0]\n",
        "cases = 23\n",
        "sampling_rate = 256  # in Hz\n",
        "time = np.linspace(0, 1, sampling_rate, endpoint=False)  # 1 second of data\n",
        "\n",
        "def plot_fft(signals, num_samples=1, show_only_average=False, x_lim_max=60):\n",
        "     for i in range(num_samples):\n",
        "          if not show_only_average:\n",
        "               plt.figure(figsize=(14, 14))\n",
        "          j = 0\n",
        "          for ch in range(signals[i].shape[0]):\n",
        "               # Compute the FFT\n",
        "               fft_values = fft(signals[i][ch])\n",
        "               N = len(fft_values)\n",
        "\n",
        "               # Compute the two-sided spectrum\n",
        "               P2 = np.abs(fft_values/N)\n",
        "\n",
        "               # Compute the single-sided spectrum (since the spectrum is symmetric)\n",
        "               P1 = P2[0:N//2+1]\n",
        "               P1[1:-1] = 2*P1[1:-1]\n",
        "\n",
        "               # Define the frequency axis\n",
        "               f = sampling_rate*np.arange(0,(N//2+1))/N\n",
        "               if not show_only_average:\n",
        "                    # Plot the single-sided amplitude spectrum\n",
        "                    plt.subplot(7, 4, ch+1)\n",
        "                    plt.plot(f, P1)\n",
        "                    plt.title(f'Channel {ch+1}')\n",
        "                    plt.xlabel('Frequency (Hz)')\n",
        "                    plt.ylabel('Amplitude')\n",
        "                    plt.xlim([0, x_lim_max])\n",
        "                    #plt.ylim([0, 0.1])\n",
        "\n",
        "          # Create an additional subplot for the average of all channels\n",
        "          avg_fft = np.zeros((N//2+1,))\n",
        "          for ch in range(signals[i].shape[0]):\n",
        "               fft_values = fft(signals[i][ch])\n",
        "               N = len(fft_values)\n",
        "               P2 = np.abs(fft_values/N)\n",
        "               P1 = P2[0:N//2+1]\n",
        "               P1[1:-1] = 2*P1[1:-1]\n",
        "               avg_fft += P1\n",
        "          avg_fft /= signals[i].shape[0]\n",
        "\n",
        "          if show_only_average:\n",
        "               plt.figure(figsize=(6, 2))\n",
        "          else:\n",
        "               plt.subplot(7, 4, ch+2)\n",
        "\n",
        "          plt.plot(f, avg_fft)\n",
        "          plt.title('Average of All Channels')\n",
        "          plt.xlabel('Frequency (Hz)')\n",
        "          plt.ylabel('Amplitude')\n",
        "          plt.xlim([0, x_lim_max])\n",
        "\n",
        "          if not show_only_average:\n",
        "               plt.suptitle(f'FFT of EEG Training Signals for Example {i+1}', fontsize=16)\n",
        "          else:\n",
        "               plt.title(f'Average FFT of EEG Training Signals for Example {i+1}')\n",
        "          plt.tight_layout()\n",
        "          plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Plot the FFT\n",
        "#plot_fft(signals_train, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "U_cFkwhWZJZN"
      },
      "outputs": [],
      "source": [
        "from scipy.interpolate import interp1d\n",
        "\n",
        "# Identify and interpolate zero-variance channels\n",
        "def interpolate_zero_variance_channels(signals, verbose=False):\n",
        "   for i in range(signals.shape[0]):\n",
        "      for j in range(signals.shape[1]):\n",
        "         if np.var(signals[i, j, :]) == 0:\n",
        "            if verbose:\n",
        "               print(f\"Channel {j} in example {i} has zero variance. Interpolating...\")\n",
        "\n",
        "            # Find channels that have non-zero variance\n",
        "            non_zero_channels = [k for k in range(signals.shape[1]) if np.var(signals[i, k, :]) != 0]\n",
        "\n",
        "            # If there are no non-zero channels, we can't do interpolation\n",
        "            if not non_zero_channels:\n",
        "               if verbose:\n",
        "                  print(f\"No non-zero channels found for example {i}. Cannot interpolate.\")\n",
        "               continue\n",
        "\n",
        "            # Create an interpolation function based on the non-zero channels\n",
        "            f = interp1d(non_zero_channels, signals[i, non_zero_channels, :], axis=0, fill_value='extrapolate')\n",
        "\n",
        "            # Use the interpolation function to estimate the signal for the zero-variance channel\n",
        "            signals[i, j, :] = f(j)\n",
        "\n",
        "   return signals\n",
        "\n",
        "# Interpolate zero-variance channels\n",
        "inter_signals_train = interpolate_zero_variance_channels(signals_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "bUAtenaWZJZN",
        "outputId": "e7d4dadf-1c5b-49fd-9f76-083189c0b7bc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Show the positive and negative class distribution\n",
        "def plot_class_distribution(labels):\n",
        "    # Count the number of positive and negative examples\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    counts_dict = dict(zip(unique, counts))\n",
        "\n",
        "    # Convert dictionary keys and values to lists\n",
        "    keys = list(counts_dict.keys())\n",
        "    values = list(counts_dict.values())\n",
        "\n",
        "    # Plot the class distribution\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.bar(keys, values, color=['blue', 'red'])\n",
        "    plt.xticks([0, 1], ['Negative', 'Positive'])\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Class Distribution')\n",
        "    plt.show()\n",
        "\n",
        "plot_class_distribution(labels_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqPvAH_CZJZN"
      },
      "outputs": [],
      "source": [
        "# Implement Butterworth filter\n",
        "from scipy.signal import butter, lfilter\n",
        "\n",
        "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return b, a\n",
        "\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n",
        "\n",
        "def apply_filter(signals, lowcut, highcut, fs):\n",
        "    filtered_signals = np.zeros(signals.shape)\n",
        "    for i in range(signals.shape[0]):\n",
        "        filtered_signals[i] = butter_bandpass_filter(signals[i], lowcut, highcut, fs)\n",
        "    return filtered_signals\n",
        "\n",
        "# Filter the signals from 1 to 40 Hz\n",
        "train_filtered = apply_filter(inter_signals_train, 1, 40, sampling_rate)\n",
        "#plot_eeg_signals(train_filtered)\n",
        "\n",
        "# Plot the FFT of the filtered signals\n",
        "plot_fft(train_filtered, 5, True, x_lim_max=130)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jme2OnnJZJZN",
        "outputId": "d767bb50-90bf-402f-b834-90cb7628718c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data shape is (37666, 23, 256)\n",
            "Labels shape for train is (37666,)\n",
            "Length of the first filtered signal is 256\n"
          ]
        }
      ],
      "source": [
        "print('Train data shape is {}'.format(train_filtered.shape))\n",
        "print('Labels shape for train is {}'.format(labels_train.shape))\n",
        "print(f'Length of the first filtered signal is {len(train_filtered[1, 0])}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "tags": [
          "parameters"
        ],
        "id": "PYrI1blFZJZN"
      },
      "outputs": [],
      "source": [
        "# Implement epoching to divide the data into X-ms epochs (we use 1000ms which is the same as the original data)\n",
        "def epoch_signals(signals, start, end, sample_rate):\n",
        "      # Calculate the number of samples per epoch\n",
        "      samples_per_epoch = int((abs(start) + abs(end)) * (sample_rate / 1000))\n",
        "\n",
        "      # Initialize an empty list to store the epochs\n",
        "      epochs = []\n",
        "\n",
        "      # Iterate over the signals\n",
        "      for signal in signals:\n",
        "            # Iterate over the channels\n",
        "            for channel in signal:\n",
        "                  # Iterate over the channel in steps of samples_per_epoch\n",
        "                  for i in range(0, len(channel), samples_per_epoch):\n",
        "                        # Extract the epoch\n",
        "                        epoch = channel[i:i+samples_per_epoch]\n",
        "\n",
        "                        # If the epoch is shorter than samples_per_epoch, discard it\n",
        "                        if len(epoch) < samples_per_epoch:\n",
        "                              continue\n",
        "\n",
        "                        # Add the epoch to the list of epochs\n",
        "                        epochs.append(epoch)\n",
        "\n",
        "      # Convert the list of epochs to a numpy array and reshape it to match the original data shape\n",
        "      epochs = np.array(epochs).reshape(len(signals), -1, samples_per_epoch)\n",
        "\n",
        "      return epochs\n",
        "\n",
        "start = 0\n",
        "end = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se5EFqSCZJZO"
      },
      "source": [
        "Several of the false cases have a skewed baseline. Below we will calculate the baseline for each channel for each case and reset the baseline to zero. This will allow the models to better compare the signals.\n",
        "\n",
        "See https://www.youtube.com/watch?v=zDTsePeDlwo&t=745s for a discussion of the baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "UTGJe6K_ZJZP"
      },
      "outputs": [],
      "source": [
        "# Create a function to apply baseline correction to the data (skip epoching)\n",
        "\n",
        "def apply_baseline_correction(signals, baseline_start, baseline_end, sample_rate):\n",
        "      # Calculate the number of samples per baseline\n",
        "      samples_per_baseline = int((abs(baseline_start) + abs(baseline_end)) * (sample_rate / 1000))\n",
        "\n",
        "      # Iterate over the signals\n",
        "      for i in range(signals.shape[0]):\n",
        "         # Iterate over the channels\n",
        "         for j in range(signals.shape[1]):\n",
        "               # Iterate over the channel in steps of samples_per_baseline\n",
        "               for k in range(0, len(signals[i, j]), samples_per_baseline):\n",
        "                  # Extract the baseline\n",
        "                  baseline = signals[i, j, k:k+samples_per_baseline]\n",
        "\n",
        "                  # If the baseline is shorter than samples_per_baseline, discard it\n",
        "                  if len(baseline) < samples_per_baseline:\n",
        "                     continue\n",
        "\n",
        "                  # Subtract the mean of the baseline from the signal\n",
        "                  signals[i, j, k:k+samples_per_baseline] -= np.mean(baseline)\n",
        "\n",
        "      return signals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "ntpGnelIZJZP"
      },
      "outputs": [],
      "source": [
        "# No epoching is necessary\n",
        "train_corrected = apply_baseline_correction(train_filtered, 0, 100, sampling_rate)\n",
        "\n",
        "val_inter_signals = interpolate_zero_variance_channels(signals_val, verbose=False)\n",
        "val_filtered = apply_filter(val_inter_signals, 1, 40, sampling_rate)\n",
        "val_corrected = apply_baseline_correction(val_filtered, 0, 100, sampling_rate)\n",
        "\n",
        "test_inter_signals = interpolate_zero_variance_channels(signals_test, verbose=False)\n",
        "test_filtered = apply_filter(test_inter_signals, 1, 40, sampling_rate)\n",
        "test_corrected = apply_baseline_correction(test_filtered, 0, 100, sampling_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xqU6xNHZJZP"
      },
      "outputs": [],
      "source": [
        "print('Train data shape is {}'.format(train_corrected.shape))\n",
        "plot_eeg_signals(train_corrected, examples=range(0, 2), channels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "RdR1ahtnZJZQ",
        "outputId": "642c526d-54a0-461a-a796-f7e27e13be0a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"b066db15-fe5e-459f-8ae6-77e22eee435f\" class=\"plotly-graph-div\" style=\"height:400px; width:400px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b066db15-fe5e-459f-8ae6-77e22eee435f\")) {                    Plotly.newPlot(                        \"b066db15-fe5e-459f-8ae6-77e22eee435f\",                        [{\"marker\":{\"color\":[\"blue\",\"red\"]},\"name\":\"Mean Range of Values\",\"x\":[\"True\",\"False\"],\"y\":[300.67576253767675,130.22186753828677],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Mean Range of Values for All Channels\\u003cbr\\u003e Averaged Together Over All Examples\",\"x\":0.5},\"xaxis\":{\"title\":{\"text\":\"True and False Examples\"}},\"yaxis\":{\"title\":{\"text\":\"Mean Range of Values\"}},\"barmode\":\"group\",\"width\":400,\"height\":400},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b066db15-fe5e-459f-8ae6-77e22eee435f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Show the mean range of values for all channels averaged together over all examples\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "def plot_mean_range(signals):\n",
        "    # Create a figure\n",
        "    fig = go.Figure()\n",
        "    example_means_true = []\n",
        "    example_means_false = []\n",
        "\n",
        "    for i in range(signals.shape[0]):\n",
        "        # Calculate the mean range of values for all channels\n",
        "        mean_range = np.mean(np.ptp(signals[i], axis=1))\n",
        "        if labels_train[i] == 1:\n",
        "            example_means_true.append(mean_range)\n",
        "        else:\n",
        "            example_means_false.append(mean_range)\n",
        "\n",
        "    trues_mean = np.mean(example_means_true)\n",
        "    falses_mean = np.mean(example_means_false)\n",
        "\n",
        "    # Plot the two means a bar chart\n",
        "    fig.add_trace(go.Bar(x=['True', 'False'], y=[trues_mean, falses_mean], name='Mean Range of Values', marker_color= ['blue', 'red']))\n",
        "\n",
        "    # Set the figure layout\n",
        "    fig.update_layout(\n",
        "        title='Mean Range of Values for All Channels<br> Averaged Together Over All Examples',\n",
        "        title_x=0.5,\n",
        "        xaxis_title='True and False Examples',\n",
        "        yaxis_title='Mean Range of Values',\n",
        "        barmode='group',\n",
        "        width=400,\n",
        "        height=400\n",
        "    )\n",
        "\n",
        "    # Show the figure\n",
        "    fig.show()\n",
        "\n",
        "plot_mean_range(train_corrected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLBniWcBZJZQ"
      },
      "source": [
        "The above chart demonstrates that perhaps large value ranges (transients) are actual seizure-predictive features. However, they could also be blinking or other movement artifacts, and the real range of the signals is much smaller (closer to the range of the negative cases)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "4zC22uF6ZJZQ"
      },
      "outputs": [],
      "source": [
        "# Implement a function to remove channels with extreme values (e.g. > 1000 µV)\n",
        "def remove_extreme_channels(signals, threshold):\n",
        "    # Iterate over the signals\n",
        "    for i in range(signals.shape[0]):\n",
        "        # Iterate over the channels\n",
        "        for j in range(signals.shape[1]):\n",
        "            # If the channel has an extreme value, set it to zero\n",
        "            if np.max(np.abs(signals[i, j])) > threshold:\n",
        "                signals[i, j] = range(signals[i, j].shape[0])\n",
        "\n",
        "    return signals\n",
        "\n",
        "# Remove channels with extreme values\n",
        "#train_cleaned = remove_extreme_channels(train_corrected, 300)\n",
        "\n",
        "# Interpolate zero-variance channels\n",
        "#train_cleaned = interpolate_zero_variance_channels(train_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "TCnjCStWZJZQ"
      },
      "outputs": [],
      "source": [
        "# Implement a function to sum the channels together\n",
        "def average_channels(signals):\n",
        "      # Sum the channels together\n",
        "      summed_signals = np.sum(signals, axis=1)\n",
        "\n",
        "      return summed_signals\n",
        "\n",
        "# Average the channels together\n",
        "#train_averaged = average_channels(train_corrected)\n",
        "#val_averaged = average_channels(val_corrected)\n",
        "#test_averaged = average_channels(test_corrected)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAmK17zoZJZQ"
      },
      "source": [
        "So far, epoching, summing channels, and interpolating extreme channels have not improved the accuracy of our model.\n",
        "\n",
        "However, interpolating missing channels, filtering at 40Hz, and doing baseline correction between 0 and 100 samples have improved the accuracy of our model.\n",
        "\n",
        "XDAWN was not applied to the averaged channels for obvious reasons, but it has been applied to all other techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DtCw532ZJZQ"
      },
      "source": [
        "Below we are implementing XDAWN covariance matrix estimation. We will use the XDAWN algorithm to estimate the covariance matrix of the data to reduce dimensionality, training the classifier on the matrix as opposed to the raw data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "owJiaxpmZJZQ"
      },
      "outputs": [],
      "source": [
        "from pyriemann.estimation import XdawnCovariances\n",
        "\n",
        "# Initialize the XdawnCovariances object\n",
        "xdawn = XdawnCovariances(nfilter=5, applyfilters=False)  # nfilter is the number of spatial filters\n",
        "\n",
        "xdawn.fit(train_corrected, labels_train)\n",
        "\n",
        "train_xdawn = xdawn.transform(train_corrected)\n",
        "val_xdawn = xdawn.transform(val_corrected)\n",
        "test_xdawn = xdawn.transform(test_corrected)\n",
        "\n",
        "# *_xdawn now contain the XDAWN covariance matrices for each epoch per dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDjYYbR_ZJZQ",
        "outputId": "5fd1b09f-a87d-48e3-f1e9-0f2d8fdd7d46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XDAWN train 2D shape: (37666, 1089)\n"
          ]
        }
      ],
      "source": [
        "# Rename the training and validation sets\n",
        "X_train, X_test, y_train, y_test = train_xdawn, val_xdawn, labels_train, labels_val\n",
        "\n",
        "# Reshape the XDAWN covariance matrices into a 2D array for the classifier\n",
        "X_train_2d = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test_2d = X_test.reshape(X_test.shape[0], -1)\n",
        "test_2D = test_xdawn.reshape(test_xdawn.shape[0], -1)\n",
        "print('XDAWN train 2D shape:', X_train_2d.shape)\n",
        "\n",
        "# Combine the training and validation sets\n",
        "X_train_all = np.vstack((X_train_2d, X_test_2d))\n",
        "y_train_all = np.hstack((y_train, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwJm_CvAZJZS",
        "outputId": "79c0cb46-1f30-4896-e28d-c245c48b081d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep and Wide summary:\n",
            "Model: \"deep_and_wide\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_21 (InputLayer)       [(None, 1089)]               0         []                            \n",
            "                                                                                                  \n",
            " normalization_20 (Normaliz  (None, 1089)                 2179      ['input_21[0][0]']            \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " Dense_1 (Dense)             (None, 32)                   34880     ['normalization_20[0][0]']    \n",
            "                                                                                                  \n",
            " dropout_180 (Dropout)       (None, 32)                   0         ['Dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_160 (B  (None, 32)                   128       ['dropout_180[0][0]']         \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " Dense_2 (Dense)             (None, 32)                   1056      ['batch_normalization_160[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dropout_181 (Dropout)       (None, 32)                   0         ['Dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_161 (B  (None, 32)                   128       ['dropout_181[0][0]']         \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " Dense_3 (Dense)             (None, 32)                   1056      ['batch_normalization_161[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dropout_182 (Dropout)       (None, 32)                   0         ['Dense_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_162 (B  (None, 32)                   128       ['dropout_182[0][0]']         \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " Dense_4 (Dense)             (None, 32)                   1056      ['batch_normalization_162[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dropout_183 (Dropout)       (None, 32)                   0         ['Dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_163 (B  (None, 32)                   128       ['dropout_183[0][0]']         \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " Dense_5 (Dense)             (None, 32)                   1056      ['batch_normalization_163[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dropout_184 (Dropout)       (None, 32)                   0         ['Dense_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_164 (B  (None, 32)                   128       ['dropout_184[0][0]']         \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " Dense_6 (Dense)             (None, 128)                  4224      ['batch_normalization_164[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dropout_185 (Dropout)       (None, 128)                  0         ['Dense_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_165 (B  (None, 128)                  512       ['dropout_185[0][0]']         \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " Dense_7 (Dense)             (None, 128)                  16512     ['batch_normalization_165[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dropout_186 (Dropout)       (None, 128)                  0         ['Dense_7[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_20 (Concatenat  (None, 1345)                 0         ['normalization_20[0][0]',    \n",
            " e)                                                                  'dropout_185[0][0]',         \n",
            "                                                                     'dropout_186[0][0]']         \n",
            "                                                                                                  \n",
            " dense_19 (Dense)            (None, 1)                    1346      ['concatenate_20[0][0]']      \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 64517 (252.02 KB)\n",
            "Trainable params: 61762 (241.26 KB)\n",
            "Non-trainable params: 2755 (10.77 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers, Model, optimizers, callbacks\n",
        "import tensorflow as tf\n",
        "\n",
        "input_ = layers.Input(shape=X_train_2d.shape[1])\n",
        "# Define the layers\n",
        "normalizer = layers.Normalization()\n",
        "normalizer.adapt(X_train_2d)\n",
        "normalization_layer = normalizer(input_)\n",
        "hidden_layer1 = layers.Dense(32, activation=\"relu\", kernel_initializer='he_normal', name='Dense_1')(normalization_layer)\n",
        "dropout1 = layers.Dropout(0.25)(hidden_layer1)\n",
        "batch_norm_1 = layers.BatchNormalization()(dropout1)\n",
        "hidden_layer2 = layers.Dense(32, activation=\"relu\", kernel_initializer='he_normal', name='Dense_2')(batch_norm_1)\n",
        "dropout2 = layers.Dropout(0.25)(hidden_layer2)\n",
        "batch_norm_2 = layers.BatchNormalization()(dropout2)\n",
        "hidden_layer3 = layers.Dense(32, activation=\"relu\", kernel_initializer='he_normal', name='Dense_3')(batch_norm_2)\n",
        "dropout3 = layers.Dropout(0.25)(hidden_layer3)\n",
        "batch_norm_3 = layers.BatchNormalization()(dropout3)\n",
        "hidden_layer4 = layers.Dense(32, activation=\"relu\", kernel_initializer='he_normal', name='Dense_4')(batch_norm_3)\n",
        "dropout4 = layers.Dropout(0.25)(hidden_layer4)\n",
        "batch_norm_4 = layers.BatchNormalization()(dropout4)\n",
        "hidden_layer5 = layers.Dense(32, activation=\"relu\", kernel_initializer='he_normal', name='Dense_5')(batch_norm_4)\n",
        "dropout5 = layers.Dropout(0.25)(hidden_layer5)\n",
        "batch_norm_5 = layers.BatchNormalization()(dropout5)\n",
        "hidden_layer6 = layers.Dense(128, activation=\"relu\", kernel_initializer='he_normal', name='Dense_6')(batch_norm_5)\n",
        "dropout6 = layers.Dropout(0.5)(hidden_layer6)\n",
        "batch_norm_6 = layers.BatchNormalization()(dropout6)\n",
        "hidden_layer7 = layers.Dense(128, activation=\"relu\", kernel_initializer='he_normal', name='Dense_7')(batch_norm_6)\n",
        "dropout7 = layers.Dropout(0.5)(hidden_layer7)\n",
        "batch_norm_7 = layers.BatchNormalization()(dropout7)\n",
        "hidden_layer8 = layers.Dense(128, activation=\"relu\", kernel_initializer='he_normal', name='Dense_8')(batch_norm_7)\n",
        "dropout8 = layers.Dropout(0.5)(hidden_layer8)\n",
        "batch_norm_8 = layers.BatchNormalization()(dropout8)\n",
        "hidden_layer9 = layers.Dense(128, activation=\"relu\", kernel_initializer='he_normal', name='Dense_9')(batch_norm_8)\n",
        "dropout9 = layers.Dropout(0.5)(hidden_layer9)\n",
        "\n",
        "concat_layer = layers.Concatenate()\n",
        "concat_layer = concat_layer([normalization_layer, dropout6,dropout7])\n",
        "output_layer = layers.Dense(1, activation='sigmoid')(concat_layer)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[input_], outputs=[output_layer])\n",
        "\n",
        "# Define the model\n",
        "deep_and_wide = Model(inputs=[input_], outputs=[output_layer], name='deep_and_wide')\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "deep_and_wide.compile(\n",
        "            optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy'],\n",
        "            sample_weight_mode=None,\n",
        "        )\n",
        "\n",
        "# Print the summary of the models\n",
        "#print('Base model summary:')\n",
        "#base.summary()\n",
        "\n",
        "#print('CNN model summary:')\n",
        "#cnn.summary()\n",
        "\n",
        "#print('Deep sequential summary:')\n",
        "#deep_sequential.summary()\n",
        "\n",
        "print('Deep and Wide summary:')\n",
        "deep_and_wide.summary()\n",
        "\n",
        "# Implement early stopping, tensorboard callbacks, and a learning rate scheduler\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy', # what to monitor\n",
        "    patience=5, # how many epochs to wait before stopping\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "\n",
        "tensorboard_callback = callbacks.TensorBoard(\n",
        "    log_dir='logs',\n",
        "    histogram_freq=1,\n",
        "    write_graph=True,\n",
        "    update_freq='epoch',\n",
        "    profile_batch=2,\n",
        "    embeddings_freq=0,\n",
        "    embeddings_metadata=None\n",
        "\n",
        ")\n",
        "\n",
        "lr_scheduler = callbacks.ReduceLROnPlateau(\n",
        "   monitor='val_accuracy',\n",
        "   factor=0.1,\n",
        "   patience=3,\n",
        "   verbose=1,\n",
        "   min_lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume we have 1000 samples and 10 time steps\n",
        "num_samples = 37666\n",
        "\n",
        "# Get the number of true and false samples\n",
        "num_true_samples = np.sum(y_train)\n",
        "num_false_samples = num_samples - num_true_samples\n",
        "\n",
        "# We'll give a higher weight to the first half of the samples\n",
        "sample_weight = np.zeros(num_samples)\n",
        "for i in range(num_samples):\n",
        "    if y_train[i] == 1:\n",
        "        sample_weight[i] = 1\n",
        "    else:\n",
        "        sample_weight[i] = 0.5\n",
        "\n",
        "\n",
        "# Train the deep_and_wide model\n",
        "deep_and_wide_history = deep_and_wide.fit(X_train_2d, y_train,\n",
        "                        validation_data=(X_test_2d, y_test),\n",
        "                        epochs=100,\n",
        "                        shuffle=True,\n",
        "                        steps_per_epoch=X_train_2d.shape[0] // 16,\n",
        "                        validation_steps=X_test_2d.shape[0] // 16,\n",
        "                        batch_size=16,\n",
        "                        callbacks=[tensorboard_callback, early_stopping, lr_scheduler])\n",
        "# Evaluate the deep_and_wide model\n",
        "deep_and_wide_accuracy = deep_and_wide.evaluate(X_test_2d, y_test)[1]\n",
        "print(f\"Deep and wide model accuracy: {deep_and_wide_accuracy * 100}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGhUqeC_c9BB",
        "outputId": "c42fe75f-3e69-4dd1-c14a-76b041d531a4"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2354/2354 [==============================] - 20s 7ms/step - loss: 0.4108 - accuracy: 0.8459 - val_loss: 0.3770 - val_accuracy: 0.8461 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "2354/2354 [==============================] - 15s 6ms/step - loss: 0.3708 - accuracy: 0.8605 - val_loss: 0.3733 - val_accuracy: 0.8579 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "2354/2354 [==============================] - 15s 7ms/step - loss: 0.3519 - accuracy: 0.8652 - val_loss: 0.4335 - val_accuracy: 0.8549 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "2354/2354 [==============================] - 16s 7ms/step - loss: 0.3413 - accuracy: 0.8682 - val_loss: 0.3434 - val_accuracy: 0.8589 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "2354/2354 [==============================] - 15s 7ms/step - loss: 0.3259 - accuracy: 0.8714 - val_loss: 0.3392 - val_accuracy: 0.8607 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "2354/2354 [==============================] - 17s 7ms/step - loss: 0.3187 - accuracy: 0.8746 - val_loss: 0.3191 - val_accuracy: 0.8700 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "2354/2354 [==============================] - 18s 8ms/step - loss: 0.3126 - accuracy: 0.8785 - val_loss: 0.3035 - val_accuracy: 0.8704 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "2354/2354 [==============================] - 16s 7ms/step - loss: 0.2996 - accuracy: 0.8826 - val_loss: 0.3194 - val_accuracy: 0.8717 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "2354/2354 [==============================] - 18s 8ms/step - loss: 0.2876 - accuracy: 0.8857 - val_loss: 0.3094 - val_accuracy: 0.8710 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "2354/2354 [==============================] - 18s 8ms/step - loss: 0.2928 - accuracy: 0.8883 - val_loss: 0.2946 - val_accuracy: 0.8775 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "2354/2354 [==============================] - 16s 7ms/step - loss: 0.2831 - accuracy: 0.8926 - val_loss: 0.2986 - val_accuracy: 0.8779 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "2354/2354 [==============================] - 16s 7ms/step - loss: 0.2858 - accuracy: 0.8913 - val_loss: 0.3079 - val_accuracy: 0.8807 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "2354/2354 [==============================] - 15s 7ms/step - loss: 0.2823 - accuracy: 0.8960 - val_loss: 0.2961 - val_accuracy: 0.8760 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "2354/2354 [==============================] - 16s 7ms/step - loss: 0.2794 - accuracy: 0.8939 - val_loss: 0.2877 - val_accuracy: 0.8800 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "2354/2354 [==============================] - 16s 7ms/step - loss: 0.2694 - accuracy: 0.8977 - val_loss: 0.3007 - val_accuracy: 0.8746 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "2354/2354 [==============================] - 15s 7ms/step - loss: 0.2658 - accuracy: 0.9006 - val_loss: 0.3429 - val_accuracy: 0.8715 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "2354/2354 [==============================] - 17s 7ms/step - loss: 0.2703 - accuracy: 0.9000 - val_loss: 0.3060 - val_accuracy: 0.8715 - lr: 0.0010\n",
            "253/253 [==============================] - 1s 4ms/step - loss: 0.3078 - accuracy: 0.8807\n",
            "Deep and wide model accuracy: 88.0683958530426%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "-oxRCUTiZJZS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "outputId": "039dff66-776c-4627-ac00-d48019eaefe7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"e34624df-b964-4ea8-afe9-da3df67baacf\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e34624df-b964-4ea8-afe9-da3df67baacf\")) {                    Plotly.newPlot(                        \"e34624df-b964-4ea8-afe9-da3df67baacf\",                        [{\"mode\":\"lines\",\"name\":\"Training Loss\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],\"y\":[0.4107568860054016,0.3708125054836273,0.35193881392478943,0.34134790301322937,0.32592055201530457,0.31867334246635437,0.31261175870895386,0.2996031641960144,0.28755438327789307,0.29278314113616943,0.2830815315246582,0.2858419716358185,0.28234004974365234,0.2794209420681,0.26941564679145813,0.2658344507217407,0.2703295648097992],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Validation Loss\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],\"y\":[0.37702086567878723,0.3733021914958954,0.433533638715744,0.3433820307254791,0.33923959732055664,0.31911203265190125,0.30349984765052795,0.3194297254085541,0.30936387181282043,0.29455047845840454,0.2986205816268921,0.30788201093673706,0.29613205790519714,0.2876965403556824,0.300667405128479,0.34287407994270325,0.305986613035202],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Training Accuracy\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],\"y\":[0.8458740711212158,0.860531210899353,0.8652324080467224,0.8681806325912476,0.871367871761322,0.8745816946029663,0.8784595131874084,0.8826029300689697,0.8857370615005493,0.8882603049278259,0.8925631046295166,0.8913413286209106,0.89604252576828,0.8938645124435425,0.8976892232894897,0.9006108641624451,0.899973452091217],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Validation Accuracy\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],\"y\":[0.8461061716079712,0.8578869104385376,0.8549107313156128,0.8588789701461792,0.8607391119003296,0.870039701461792,0.8704116940498352,0.8716517686843872,0.8710317611694336,0.877480149269104,0.877852201461792,0.8807043433189392,0.8759920597076416,0.879960298538208,0.874627947807312,0.8715277910232544,0.8715277910232544],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Learning Curves\",\"x\":0.5},\"xaxis\":{\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"title\":{\"text\":\"Value\"}},\"width\":800,\"height\":600},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e34624df-b964-4ea8-afe9-da3df67baacf');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plot the learning curves using plotly\n",
        "def plot_learning_curves(history):\n",
        "   # Create a figure\n",
        "   fig = go.Figure()\n",
        "\n",
        "    # Add the training and validation loss\n",
        "   fig.add_trace(go.Scatter(x=np.arange(1, len(history.history['loss']) + 1), y=history.history['loss'], mode='lines', name='Training Loss'))\n",
        "   fig.add_trace(go.Scatter(x=np.arange(1, len(history.history['val_loss']) + 1), y=history.history['val_loss'], mode='lines', name='Validation Loss'))\n",
        "\n",
        "    # Add the training and validation accuracy\n",
        "   fig.add_trace(go.Scatter(x=np.arange(1, len(history.history['accuracy']) + 1), y=history.history['accuracy'], mode='lines', name='Training Accuracy'))\n",
        "   fig.add_trace(go.Scatter(x=np.arange(1, len(history.history['val_accuracy']) + 1), y=history.history['val_accuracy'], mode='lines', name='Validation Accuracy'))\n",
        "\n",
        "   # Update the layout\n",
        "   fig.update_layout(\n",
        "         title='Learning Curves',\n",
        "         title_x=0.5,\n",
        "         xaxis_title='Epoch',\n",
        "         yaxis_title='Value',\n",
        "         width=800,\n",
        "         height=600\n",
        "   )\n",
        "\n",
        "   fig.show()\n",
        "\n",
        "plot_learning_curves(deep_and_wide_history)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "ZdaRu-UxZJZS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b197cf0-fa12-428e-c658-496ae4d1fd96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "253/253 [==============================] - 1s 5ms/step\n",
            "Enter the name for the predictions:deep_and_wide_non_seq_v1\n"
          ]
        }
      ],
      "source": [
        "# Predict the test data\n",
        "y_pred_deep_wide = deep_and_wide.predict(test_2D)\n",
        "y_pred_deep_wide = (y_pred_deep_wide > 0.5).astype(int)\n",
        "\n",
        "df = pd.DataFrame(y_pred_deep_wide, columns=['Predicted'])\n",
        "df.index.name = 'id'\n",
        "filename = input('Enter the name for the predictions:')\n",
        "df.to_csv(f'{filename}.csv', header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfR2xxlvZJZS"
      },
      "source": [
        "## Report (new deep and wide non-sequential model)\n",
        "\n",
        "The deep and wide model was troublesome to work with, much more unstable than the sequential models. I experimented with several different combinations of paths in the concatenate layer, and found that the deeper layers were far more consistent in their convergence. It also seemed like the shallow layers (the first few) did better with fewer neurons than the deeper layers, which means that the more complex features were being learned in the deeper layers. However, the features being learned in the shallower layers (those with 32 filters) did not make sense to the final classification, such as when I included 'dropout2' or 'dropout3' (those being the last operations on fully connected layers 2 and 3). The nature of the data seems to require a relatively deep model, but not so wide. I left additional layers in the model though they are not being used, for reference. I found that after hidden_layer6, adding any more layers with the same number of more filters caused the model to overfit the data.\n",
        "\n",
        "The kaggle score is 89.47%, so not terrible, it just needs more TLC.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "EEG_analysis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}